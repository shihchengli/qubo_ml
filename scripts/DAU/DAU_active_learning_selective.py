# This code combines DAU_train.py and DAU_predict.py to form a automatic workflow to generate new training data from previous prediction.
# This workflow apllies the strategic selection that add the top-50 reactions with the highest predicted yields into new training set in the next iteration.

import csv
import json
import sys
import time
from random import shuffle

import numpy
import requests
from sklearn.metrics import mean_absolute_error, mean_squared_error

# DAU authority setting
API_Key = "Key"
API_Access_URL = "URL"
headers = {"content-type": "application/json", "X-Api-Key": API_Key}
proxies = {}


# Expand the contribution for every feature and feature pair found in the reaction
# The contributions look like this: x_0 + 2x_1 + 4x_2 + ... _ 512x_9 - 512
# For the features with '1' in a reaction, the variable ids for linear terms and quadratic terms are found in the coding Dictionary.
def expand_binary_qubo(input, coding):
    bp = {}  # key=variable, value=coefficient
    feature_ids = [feature_id for feature_id, value in enumerate(input) if value == "1"]
    for feature_id1 in feature_ids:
        for i in range(10):  # 1, 2 ,4, 8, ... , 512    #linear term
            bp[feature_id1 * 10 + i] = 2**i
        feature_ids2 = [
            feature_id2 for feature_id2 in feature_ids if feature_id2 > feature_id1
        ]
        for feature_id2 in feature_ids2:  # quadratic term
            condition1 = coding[feature_id1]
            condition2 = coding[feature_id2]
            base_key = (
                coding.index((condition1, condition2)) * 10
            )  # from per mille to percent
            for i in range(10):
                bp[base_key + i] = 2**i
    return bp


# This function calculates Equation (7) of single reaction: (P_i-Y_i)^2
# P_i is the prediction yield of reaction i, which corresponds to Equation (6), composing of the contributions of features from linear terms and quadratic terms
# The contribution part is calculated in expand_binary_qubo().
# Y_i is the yield of reaction i.
def qubo_square(input, coding):
    reaction_yield = input[-1]
    variables = expand_binary_qubo(input, coding)
    n_feature = len(variables) / 10
    output = variables.copy()
    for key in variables.keys():
        output[key] = (variables[key] ** 2) - 2 * variables[key] * (
            reaction_yield + 512 * n_feature
        )
    keys = list(variables.keys())
    keys.sort()
    n_keys = len(keys)
    for id1 in range(n_keys):
        for id2 in range(id1 + 1, n_keys):
            output[(keys[id1], keys[id2])] = (
                2 * variables[keys[id1]] * variables[keys[id2]]
            )
    output["constant"] = (reaction_yield + 512 * n_feature) ** 2
    return output


# Sum all the terms from different reactions
def merge_qubo(qubo_list):
    output = {}
    for qubo in qubo_list:
        for key in qubo.keys():
            output[key] = output.get(key, 0) + qubo[key]
    return output


# This function read the features and yields from input csv files and output them as List.
def read_csv(filename, feature_total):
    with open(filename, "r") as f:
        csv_reader = csv.reader(f)
        header = next(csv_reader)
        id_yield = header.index("yield")
        yield ["id"] + header
        for row in csv_reader:
            row[id_yield] = (
                float(row[id_yield]) * 10
            )  # yield*10, from percent to per mille
            yield [row[0]] + row[-1 * feature_total :] + [
                row[id_yield]
            ]  # [id, features, yield]


# Transform the QUBO terms into the format requested by DAU API.
def dau_encode(objective_function, penalty_function):
    bp_terms = [{"c": objective_function["constant"], "p": []}]
    for term, coefficient in objective_function.items():
        if isinstance(term, int):
            bp_terms.append({"c": coefficient, "p": [term]})
        elif isinstance(term, tuple):
            bp_terms.append({"c": coefficient, "p": list(term)})
    bp = {"terms": bp_terms}
    if penalty_function:
        pbp_terms = [{"c": penalty_function["constant"], "p": []}]
        for term in penalty_function:
            if isinstance(term, int):
                pbp_terms.append({"c": penalty_function[term], "p": [int(term)]})
            elif isinstance(term, tuple):
                pbp_terms.append({"c": penalty_function[term], "p": list(term)})
        pbp = {"terms": pbp_terms}
    else:
        pbp = None
    return (bp, pbp)


# titles = ['cn-processed':{35:3,5,3,4,20}, 'amidation':{25:4,3,8,10}, 'aryl-scope-ligand':{40:24,8,8}, 'deoxyf':{46:4,5,37}]
# Their are 4 HTE datasets. The Value of the dictionary contains the number of features and the number in the group of one-hot encoding features
# Be cautious! The scheme is different from the original file, but can be generated by concatenating two related files.
# For example, the training, validation, and testing datasets are csv files composing of 37 columns for the C-N cross coupling dataset.
# Column 1: Substrate_SMILES
# Column 2: Yield
# Column 3-37: Features

# The variable dir and feature_total should be changed according to the datasets.
dir = "cn-processed"
feature_total = 35

iteration = int(sys.argv[1])
version = "v1"

# There are 5 folds of different starting training sets.
# For the first iteration. The filename should be manually changed to 'fold_<fold>.csv'.
for fold in range(5):
    # Define the input and output filenames
    title = "fold_" + str(fold) + "_iter_" + str(iteration) + "_" + version
    response_filename = dir + "/" + title + "_response.txt"
    coefficient_filename = dir + "/" + title + "_coefficient_0.csv"
    if iteration == 0:
        train_filename = dir + "/fold_" + str(fold) + ".csv"
    else:
        train_filename = dir + "/" + title + ".csv"

    # Read the training dataset and split 10% data to generate validation set for penalty function
    inputs = read_csv(train_filename, feature_total)
    header = next(inputs)
    inputs = list(inputs)
    shuffle(inputs)
    penalty_num = int(len(inputs) / 10)

    # Build the mapping table for features according to the structural encoding
    coding = header[-1 * feature_total :]
    groups = {}
    for condition_id, condition in enumerate(coding):
        if condition_id < 3:
            groups[condition] = 0  #'substrate_halide': 3
        elif condition_id < 8:
            groups[condition] = 1  #'substrate_variation': 5
        elif condition_id < 11:
            groups[condition] = 2  #'base': 3
        elif condition_id < 15:
            groups[condition] = 3  #'ligand': 4
        else:
            groups[condition] = 4  #'additive': 20

    # Construct the quadratic terms
    n_groups = len(set(groups.values()))  # =5
    for group1 in range(n_groups):
        for group2 in range(group1 + 1, n_groups):
            groups1 = [
                condition for condition, group in groups.items() if group == group1
            ]
            groups2 = [
                condition for condition, group in groups.items() if group == group2
            ]
            for condition1 in groups1:
                for condition2 in groups2:
                    coding.append((condition1, condition2))

    # print(len(coding))  #For C-N dataset: 418 = 35 + 15 + 9 + 12 + 60 + 15 + 20 + 100 + 12 + 60 + 80

    # Generate penalty term that utilize validation dataset
    pen_qubo_list = []
    for input in inputs[:penalty_num]:
        pen_qubo = qubo_square(input[1:], coding)
        pen_qubo_list.append(pen_qubo)

    # Generate objective term that utilize training dataset
    obj_qubo_list = []
    for input in inputs[penalty_num:]:
        obj_qubo = qubo_square(input[1:], coding)
        obj_qubo_list.append(obj_qubo)

    # print(len(inputs))

    # Encode the terms into the input form requested by DAU
    penalty_function = merge_qubo(pen_qubo_list)
    objective_function = merge_qubo(obj_qubo_list)
    bp, pbp = dau_encode(objective_function, penalty_function)

    # Build the request to DAU server
    request = {
        "fujitsuDA3": {
            # "time_limit_sec": int #Default=10
            # "num_output_solution": int #Default=5
            # "one_way_one_hot_groups": v_1w1h
            # "two_way_one_hot_groups": v_2w1h
        },
        "binary_polynomial": bp,
        "penalty_binary_polynomial": pbp,
        # "inequalities": [ineq1, ineq2, ...]
    }

    # Post request to DAU server
    calling_API = requests.post(
        API_Access_URL + "/async/qubo/solve",
        json.dumps(request),
        headers=headers,
        proxies=proxies,
    )
    job_id = calling_API.json()["job_id"]
    print(job_id)

    # Wait for the results
    time.sleep(60)

    # Get the results
    response = requests.get(
        API_Access_URL + "/async/jobs/result/" + job_id,
        headers=headers,
        proxies=proxies,
    )
    # print(response.json())

    # Record the results
    with open(response_filename, "w", newline="") as f:
        print(response.json(), file=f)

    delete = requests.delete(
        API_Access_URL + "/async/jobs/result/" + job_id,
        headers=headers,
        proxies=proxies,
    )

    # Parse the results, calcuate and output the contribution of each feature into csv files
    for i in range(1):
        solution = response.json()["qubo_solution"]["solutions"][i]["configuration"]
        coefficient = {}
        for id1, condition in enumerate(coding):
            if str(10 * id1) in solution:
                coefficient[condition] = 0
                for j in range(10):
                    if solution.get(str(10 * id1 + j), False):
                        coefficient[condition] += 2**j
        with open(coefficient_filename, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["variable_name", "coefficient", "yield (scale:*10)"])
            for key, value in coefficient.items():
                writer.writerow([key, value])

# Prediction part

# Initialize the performance metrices
mae_all = []
rmse_all = []
cor_coef_all = []
top_5 = []
top_10 = []
top_15 = []
top_20 = []

# Analyze all the 5 results from DAU
for fold in range(5):
    # Define input and output filenames and read the testing sets consisting of all the reactions not in the training and validation sets
    title = "fold_" + str(fold) + "_iter_" + str(iteration) + "_" + version
    test_filename = dir + "/" + dir + "_features.csv"
    coefficient_filename = dir + "/" + title + "_coefficient_0.csv"
    inputs = read_csv(test_filename, feature_total)
    header = next(inputs)[-1 * feature_total :]
    inputs = list(inputs)
    real = [input[-1] / 10 for input in inputs]
    real_ids = list(numpy.argsort(real))  # sort the reactions by the real yield
    real_ids.reverse()
    top_real = [
        real[real_id] for real_id in real_ids[:20]
    ]  # find the reactions with top-20 yield rates

    # Read the resulting contributions for each feature and feature pairs
    with open(coefficient_filename, "r") as f:
        reader = csv.reader(f)
        c_header = next(reader)
        coefficient = {rows[0]: int(rows[1]) for rows in reader}

    # Start prediction
    predict = []
    predict_file = dir + "/" + title + "_yield_0.csv"
    with open(predict_file, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["yield"] + ["prediction"])
        for input in inputs:
            real_yield = input[-1] / 10  # the last column in input data is yield
            prediction = 0
            feature_ids = [
                feature_id for feature_id, value in enumerate(input[1:]) if value == "1"
            ]
            for id1 in feature_ids:
                condition1 = header[id1]
                prediction += coefficient.get(condition1, 512) - 512  # linear term
                id2s = [id2 for id2 in feature_ids if id2 > id1]
                for id2 in id2s:
                    condition2 = header[id2]
                    prediction += (
                        coefficient.get(str((condition1, condition2)), 512) - 512
                    )  # quadratic term
            prediction = prediction / 10
            predict.append(prediction)

        # Find the reactions with top predicted yield rates
        sorted_ids = list(numpy.argsort(predict))
        sorted_ids.reverse()

        if iteration == 0:
            train_filename = dir + "/fold_" + str(fold) + ".csv"
        else:
            train_filename = dir + "/" + title + ".csv"

        # Generate the next training file by adding 50 reactions with top-50 predicted yield rates that are not in the training set
        next_train_file = (
            dir
            + "/fold_"
            + str(fold)
            + "_iter_"
            + str(iteration + 1)
            + "_"
            + version
            + ".csv"
        )
        with open(next_train_file, "w", newline="") as f1:
            train_writer = csv.writer(f1)
            train_inputs = read_csv(train_filename, feature_total)
            header = next(train_inputs)[-1 * feature_total :]
            train_writer.writerow(["", "yield"] + header)
            train_id = []
            for row in train_inputs:
                train_writer.writerow([row[0], row[-1] / 10] + row[1:-1])
                train_id.append(row[0])
            added = 0
            for sorted_id in sorted_ids:
                if added == 50:
                    break
                if inputs[sorted_id][0] not in train_id:
                    train_writer.writerow(
                        [inputs[sorted_id][0], inputs[sorted_id][-1] / 10]
                        + inputs[sorted_id][1:-1]
                    )
                    added += 1

        for sorted_id in sorted_ids:
            writer.writerow([inputs[sorted_id][-1] / 10] + [predict[sorted_id]])

    # Output the performance of this iteration
    metrics_file = dir + "/" + title + "_metrics_0.csv"
    with open(metrics_file, "w", newline="") as f:
        writer = csv.writer(f)

        top5 = 0
        for id_5 in sorted_ids[:5]:
            if real[id_5] in top_real[:5]:
                top5 += 1
        writer.writerow(["top 5", top5])
        top_5.append(top5)

        top10 = 0
        for id_10 in sorted_ids[:10]:
            if real[id_10] in top_real[:10]:
                top10 += 1
        writer.writerow(["top 10", top10])
        top_10.append(top10)

        top15 = 0
        for id_15 in sorted_ids[:15]:
            if real[id_15] in top_real[:15]:
                top15 += 1
        writer.writerow(["top 15", top15])
        top_15.append(top15)

        top20 = 0
        for id_20 in sorted_ids[:20]:
            if real[id_20] in top_real[:20]:
                top20 += 1
        writer.writerow(["top 20", top20])
        top_20.append(top20)

        mae = mean_absolute_error(real, predict)
        rmse = mean_squared_error(real, predict)
        cor_coef = numpy.cov(real, predict)[0, 1] / (
            numpy.std(real) * numpy.std(predict)
        )
        mae_all.append(mae)
        rmse_all.append(rmse)
        cor_coef_all.append(cor_coef)
        writer.writerow(["mae", mae])
        writer.writerow(["rmse", rmse])
        writer.writerow(["cor_coef", cor_coef])


print("fold", fold, "iter", iteration)

# Output averaged performance starting from 5 different folds in the same iteration
merge_metrics_file = dir + "/iter_" + str(iteration) + "_metrics_0.csv"
with open(merge_metrics_file, "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["top 5", sum(top_5) / 5])
    writer.writerow(["top 10", sum(top_10) / 5])
    writer.writerow(["top 15", sum(top_15) / 5])
    writer.writerow(["top 20", sum(top_20) / 5])
    writer.writerow(["mae", sum(mae_all) / 5])
    writer.writerow(["rmse", sum(rmse_all) / 5])
    writer.writerow(["cor_coef", sum(cor_coef_all) / 5])
